{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pw = Pdist(data=datafile(\"data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "We developed an iterative segmenter.\n",
    "\n",
    "##### Class Entry: \n",
    "Each entry in the chart has four components: Entry(word, start-position, log-probability, back-pointer), the back-pointer in each entry links it to a previous entry that it extends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entry():\n",
    "    def __init__(self, word, end, logprob, backptr):\n",
    "        self.word = word\n",
    "        self.end = end\n",
    "        self.logprob = logprob\n",
    "        self.backptr = backptr\n",
    "    def __lt__(self, other):\n",
    "        if self.logprob>other.logprob:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the matched word start with char\n",
    "def inputMatch(self, position, text, prevEntry):\n",
    "    entryList = []\n",
    "    char = text[position]\n",
    "    #for all the words within the max length\n",
    "    for i in range( min((self.Pw).maxLength, len(text)-position) ):\n",
    "        word = text[position:position+i+1]\n",
    "        if word in (self.Pw).keys():\n",
    "            if prevEntry is None:\n",
    "                logprob = log10(self.Pw(word))\n",
    "            else:\n",
    "                logprob = log10(self.cPw(word, prevEntry.word))\n",
    "            end = position + i+1\n",
    "            entryList.append(Entry(word, end, logprob, None))\n",
    "    # return the words with coming 2 chars if no matched\n",
    "    if len(entryList) == 0:\n",
    "        logprob = log10(self.Pw(char))\n",
    "        end = position + 1\n",
    "        entryList.append(Entry(char, end, logprob, None))\n",
    "        # entryList.append(Entry(char, end, logprob, None))\n",
    "        # entryList.append(Entry(char, end, logprob, None))\n",
    "    return entryList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iteratively fill in chart[i] for all i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = [None]*len(text)\n",
    "while len(entryHeap) != 0:\n",
    "    entry = heapq.heappop(entryHeap)\n",
    "    endindex = entry.end - 1\n",
    "    if chart[endindex] is not None:\n",
    "        preventry = chart[endindex]\n",
    "        if entry.logprob > preventry.logprob:\n",
    "            chart[endindex] = entry\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        chart[endindex] = entry\n",
    "        #if the text does not finish\n",
    "        if (endindex+1) < len(text):\n",
    "            entryList = self.inputMatch(endindex+1, text, chart[endindex])\n",
    "            for newEntry in entryList:\n",
    "                newStandardEntry = Entry(newEntry.word, newEntry.end, newEntry.logprob+entry.logprob, endindex)\n",
    "                if any(newStandardEntry == existEntry for existEntry in entryHeap) is not True:\n",
    "                    heapq.heappush(entryHeap, newStandardEntry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the best segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalEntry = chart[-1]\n",
    "currentEntry = finalEntry\n",
    "segmentation = []\n",
    "segmentation.append(currentEntry.word)\n",
    "while currentEntry.backptr is not None:\n",
    "    currentEntry = chart[currentEntry.backptr]\n",
    "    segmentation.append(currentEntry.word)\n",
    "segmentation.reverse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigram model and Smoothing\n",
    "We tried Laplace Add-1 smoothing as well as JM smoothing for a better F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cPw(self, word, prev):\n",
    "        \"Conditional probability of word, given previous word.\"\n",
    "        #Laplace Add-1 smoothing\n",
    "        # if prev + ' ' + word in self.P2w:\n",
    "        #     if prev in self.Pw:\n",
    "        #         return (1 + self.P2w[prev + ' ' + word])/(len(self.Pw)+float(self.Pw[prev]))\n",
    "        #     else:\n",
    "        #         return (1 + self.P2w[prev + ' ' + word])/(len(self.Pw))\n",
    "        # else:\n",
    "        #     if prev in self.Pw:\n",
    "        #         return 1 / (len(self.Pw)+float(self.Pw[prev]))\n",
    "        #     else:\n",
    "        #         return 1 / len(self.Pw)\n",
    "        # Jelinek-Mercer smoothing\n",
    "        bigramProb = 0\n",
    "        unigramProb = 0\n",
    "        if prev + ' ' + word in self.P2w:\n",
    "            bigramProb = self.P2w[prev + ' ' + word] / float(self.Pw[prev])\n",
    "        if word in self.Pw:\n",
    "            unigramProb = self.Pw(word)\n",
    "        bigram_lambda = 0.5\n",
    "        unigram_lambda = 0.3\n",
    "        return bigram_lambda*bigramProb + unigram_lambda*unigramProb + (1-bigram_lambda-unigram_lambda)*(1/float(len(self.Pw)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd0d65d7f505299504d44dde304b2df549179c5df58ec69de47ba8c1fb69755b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
