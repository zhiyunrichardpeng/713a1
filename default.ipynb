{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白 国 良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n",
      "上午 在 这里 签字 的 是 知识 信息网 络通讯技 术和脱氧 核 糖 核 酸 生物 技术 两 个 项目 ， 同时 还 签订 了 语言 教学 交流 合作 协议 。\n",
      "这 三 个 项目 是 分别 由 国务院 发展 研究 中心 国际 技术 经济 研究所 上海 分 所 和 上海市 浦东 继续 教育 中心 ， 与 美国 知识 信息 网络 公司 、 世界 学习 组织 、 海 赛克 公司 签订 的 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw)\n",
    "output_full = []\n",
    "with open(\"data/input/dev.txt\",'r', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "precision: 0.89\n",
      "recall: 0.92\n",
      "score: 0.91\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "from zhsegment_check import precision\n",
    "from zhsegment_check import recall\n",
    "with open('data/reference/dev.out','r', encoding='UTF-8') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    pn = precision(ref_data, output_full)\n",
    "    rl = recall(ref_data, output_full)\n",
    "    print(\"precision: {:.2f}\".format(pn), file=sys.stderr)\n",
    "    print(\"recall: {:.2f}\".format(rl), file=sys.stderr)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inicialize the Heap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: below are only for demonstrating, not for running. Please ignore the error report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap=[]\n",
    "heap = self.MatchWords(0, text)\n",
    "heap.sort(key=lambda a: a[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the match word module\n",
    "In the matched input module, we use the start position of the input text, to add new words to the word candidate pool. But only those word that is occurred in the 1w text will be selected out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatchWords(self, startPosition, text):\n",
    "    entryList = []\n",
    "    L = min((self.Pw).maxLength, len(text) - startPosition)  # L = min(maxlen, j) in order to avoid long words\n",
    "    for i in range(L):\n",
    "        currword = text[startPosition : startPosition + i + 1] # the current text segmentation\n",
    "        if currword in self.Pw:\n",
    "            entryList.append((currword, startPosition, log10(self.Pw(currword)), None)) # add the current entry\n",
    "\n",
    "    # if words not in dictionary\n",
    "    if len(entryList) == 0:\n",
    "        for i in range(len(text) - startPosition):\n",
    "            if(i < 4):\n",
    "                currword = text[startPosition : startPosition + i + 1]  \n",
    "                entryList.append((currword, startPosition, log10(self.Pw(currword)), None)) \n",
    "    return entryList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the pseudocode part.\n",
    "\n",
    "The following is an example of how we construct the chart. For example, our algorithm told us \"签订\" has higher probability then “签\", so it is popped out first to fill in the index 5 position. Then the “签\" is popped out to compare with the position of \"签订\", and did not win back the position, so the position 5 is still \"签订\"'s, which will be back-pointed to by the next word \"高\" in the chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final chart[ 0 ]: chartEntry (start=中, end=1 logprob=-2.448512, backptr=None)\n",
    "\n",
    "final chart[ 1 ]: chartEntry (start=美, end=2 logprob=-5.787403, backptr=0)\n",
    "\n",
    "final chart[ 2 ]: chartEntry (start=在, end=3 logprob=-7.741467, backptr=1)\n",
    "\n",
    "final chart[ 3 ]: chartEntry (start=沪, end=4 logprob=-12.069362, backptr=2)\n",
    "\n",
    "final chart[ 4 ]: chartEntry (start=签, end=5 logprob=-16.397257, backptr=3)\n",
    "\n",
    "final chart[ 5 ]: chartEntry (start=签订, end=6 logprob=-15.744045, backptr=3)\n",
    "\n",
    "final chart[ 6 ]: chartEntry (start=高, end=7 logprob=-18.594818, backptr=5)\n",
    "\n",
    "final chart[ 7 ]: chartEntry (start=订高科, end=8 logprob=-33.327212, backptr=4)\n",
    "\n",
    "final chart[ 8 ]: chartEntry (start=科技, end=9 logprob=-21.570531, backptr=6)\n",
    "\n",
    "final chart[ 9 ]: chartEntry (start=合, end=10 logprob=-26.023365, backptr=8)\n",
    "\n",
    "final chart[ 10 ]: chartEntry (start=合作, end=11 logprob=-24.206020, backptr=8)\n",
    "\n",
    "final chart[ 12 ]: chartEntry (start=协议, end=13 logprob=-27.316431, backptr=10)\n",
    "\n",
    "中 美 在 沪 签订 高 科技 合作 协议\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (len(heap) != 0):\n",
    "\n",
    "    # pop the largest prob entry from heap\n",
    "    entry = heap[-1]\n",
    "    heap = heap[:-1]\n",
    "\n",
    "    # find endindex of current text segmentation / startindex of next text segmentation\n",
    "    endindex = entry[1] + len(entry[0]) - 1\n",
    "\n",
    "    # check chart[endindex] has preventry or not\n",
    "    if chart[endindex] is not None:\n",
    "        prevEntry = chart[endindex]\n",
    "        if entry[2] > prevEntry[2]:    # check the prob of two entries\n",
    "            chart[endindex] = entry\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        chart[endindex] = entry\n",
    "\n",
    "        entryList = self.MatchWords(endindex + 1, text)\n",
    "        for currEntry in entryList:\n",
    "            newEntry = (currEntry[0], currEntry[1], currEntry[2] + entry[2], endindex) # update prob \n",
    "            if newEntry not in heap:\n",
    "                heap.append(newEntry)\n",
    "        heap.sort(key=lambda a: a[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Found the chart and output the segmentation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use back pointer to trace the segmentation\n",
    "finalEntry = chart[-1]\n",
    "segmentation = []\n",
    "segmentation.append(finalEntry[0])\n",
    "while finalEntry[3] is not None:\n",
    "    finalEntry = chart[finalEntry[3]]\n",
    "    segmentation.append(finalEntry[0])\n",
    "segmentation.reverse()\n",
    "\n",
    "return segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra function\n",
    "we add this line \"self.maxLength = max(map(len, self.keys()))\" to count the max length of the word in 1w txt. Also, we add the function in assignment 0 which makes the unseen words a very small probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key, count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.values()))\n",
    "        self.maxLength = max(map(len, self.keys()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./(N*8000**len(k))) # #same reason as A0, avoid long words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some equations related to code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argmax the production of likelyhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(nums):\n",
    "    \"Return the product of a sequence of numbers.\"\n",
    "    return reduce(operator.mul, nums, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segmentation iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check chart[endindex] has preventry or not\n",
    "if chart[endindex] is not None:\n",
    "    prevEntry = chart[endindex]\n",
    "    if entry[2] > prevEntry[2]:    # check the prob of two entries\n",
    "        chart[endindex] = entry\n",
    "    else:\n",
    "        continue\n",
    "else:\n",
    "    chart[endindex] = entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the maximun length of the word in 1w txt, and choose the smaller one between it and the remaining length of the text we are processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = min((self.Pw).maxLength, len(text) - startPosition)  # L = min(maxlen, j) in order to avoid long words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign small probability to unknown word in 1w txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.missingfn = missingfn or (lambda k, N: 1./(N*8000**len(k))) # #same reason as A0, avoid long words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this example result, our method did not perform well:\n",
    "    \"上午 在 这里 签字 的 是 知识 信息网 络通讯技 术和脱氧 核 糖 核 酸 生物 技术 两 个 项目 ， 同时 还 签订 了 语言 教学 交流 合作 协议 。\"\n",
    "    \n",
    "A better method could be to make use of the 2w txt with the bigram consideration.\n",
    "\n",
    "We tried to use the smoothing method for the unseen words in the 1w txt, but it does not work in our code, with F1 score 0.06."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
